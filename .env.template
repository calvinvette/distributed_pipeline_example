# ImageML Pipeline Environment Configuration
# Generated template - copy to .env and configure

# === AWS Configuration ===
# AWS region for S3 and other AWS services
AWS_REGION=us-east-1
# AWS access key ID for authentication (or use IAM roles)
AWS_ACCESS_KEY_ID=
# AWS secret access key (use IAM roles in production)
AWS_SECRET_ACCESS_KEY=

# === S3 Configuration ===
# S3-compatible endpoint URL (use http://localhost:9000 for local MinIO/RustFS)
S3_ENDPOINT=http://localhost:9000

# === Pipeline Buckets ===
# S3 bucket for raw input images
RAW_BUCKET=image-raw
# S3 bucket for augmented images (Stage 1 output)
AUGMENTED_BUCKET=image-augmented
# S3 bucket for normalized images (Stage 2 output)
NORMALIZED_BUCKET=image-normalized
# S3 bucket for training data (Stage 3 input)
TRAINING_BUCKET=image-training
# S3 bucket for inference input images
INFERENCE_INPUT_BUCKET=image-inference-input
# S3 bucket for human-in-the-loop review (Stage 4 output)
HITL_BUCKET=image-hitl
# S3 bucket for final output
OUTPUT_BUCKET=image-output
# S3 bucket for manifest store
MANIFEST_STORE_BUCKET=manifest-store
# S3 bucket for MLflow artifacts
MLFLOW_ARTIFACTS_BUCKET=mlflow-artifacts

# === NVMe Staging ===
# Root directory for NVMe staging (local fast storage)
NVME_ROOT=/mnt/nvme
# Minimum free space required on NVMe in GB
NVME_MIN_GB=100

# === MLflow ===
# MLflow tracking server URL
MLFLOW_TRACKING_URL=http://localhost:5000
# MLflow model registry URL (usually same as tracking)
MLFLOW_REGISTRY_URL=http://localhost:5000
# MLflow experiment name for training runs
MLFLOW_EXPERIMENT=image-ml-training

# === Stage 1 - Ingest & Augment ===
# Stage 1 execution mode: 'poll' or 'event'
STAGE1_MODE=poll
# Stage 1 polling interval in seconds
STAGE1_POLL_INTERVAL=60
# S3 prefix for raw images in RAW_BUCKET
RAW_PREFIX=images/
# S3 prefix for augmented output
OUTPUT_PREFIX=augmented/

# === Stage 2 - Normalize & Resize ===
# Comma-separated sizes for normalization (e.g., 1080p,720p,800x600)
NORMALIZE_SIZES=1080p,720p,800x600
# S3 prefix for Stage 2 input in AUGMENTED_BUCKET
INPUT_PREFIX=augmented/

# === Stage 3 - Training ===
# Number of training epochs
EPOCHS=10
# Number of images to use per node for training
IMAGES_PER_NODE=100
# Number of training instances
INSTANCE_COUNT=1
# EC2 instance type for training
INSTANCE_TYPE=ml.m5.xlarge
# S3 prefix for training data in TRAINING_BUCKET
TRAINING_PREFIX=normalized/

# === Stage 4 - Inference ===
# Name of model in MLflow registry
MODEL_NAME=image-ml-model
# Model stage to load (Production, Staging, etc.)
MODEL_STAGE=Production
# Enable Prometheus metrics
METRICS_ENABLED=true
# Port for Prometheus metrics endpoint
METRICS_PORT=8000

# === Pipeline Configuration ===
# Dataset identifier for manifest store
DATASET_ID=default
# Default execution mode for stages: 'poll' or 'event'
MODE=poll
# Default polling interval in seconds
POLL_INTERVAL=60

# === Database ===
# PostgreSQL connection URL for manifest store (e.g., postgresql://user:pass@host:5432/db)
MANIFEST_STORE_DB_URL=
# PostgreSQL connection URL for MLflow backend store
MLFLOW_DB_URL=
